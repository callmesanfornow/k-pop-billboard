{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "\n",
    "def load_data(folder_path):\n",
    "    all_data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "\n",
    "            # Load CSV and add chart_date column\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path, engine='python', encoding='utf-8')\n",
    "            # print(f\"Loaded {file_path} with shape {df.shape}\")\n",
    "\n",
    "            all_data.append(df)\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Load data\n",
    "hot100 = load_data('./final-csv/hot-100')  # adjust path as needed\n",
    "global200 = load_data('./final-csv/global-200')  # adjust path as needed\n",
    "\n",
    "# Helper functions (use your regex-based ones)\n",
    "def is_korean(word):\n",
    "    return bool(re.search(r'[\\uac00-\\ud7af]', word))\n",
    "\n",
    "def is_english(word):\n",
    "    return bool(re.search(r'[a-zA-Z]', word))\n",
    "\n",
    "def get_korean_ratio(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', str(text))\n",
    "    if not words:\n",
    "        return 0\n",
    "    korean_count = sum(1 for w in words if is_korean(w))\n",
    "    return korean_count / len(words)\n",
    "\n",
    "def get_english_ratio(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', str(text))\n",
    "    if not words:\n",
    "        return 0\n",
    "    english_count = sum(1 for w in words if is_english(w))\n",
    "    return english_count / len(words)\n",
    "\n",
    "def count_filler_words(text, filler_words):\n",
    "    words = re.findall(r'\\b\\w+\\b', str(text).lower())\n",
    "    return sum(1 for w in words if w in filler_words)\n",
    "import re\n",
    "\n",
    "def is_korean(word):\n",
    "    return bool(re.search(r'[\\uac00-\\ud7af]', word))\n",
    "\n",
    "def is_english(word):\n",
    "    return bool(re.search(r'[a-zA-Z]', word))\n",
    "\n",
    "def classify_word_regex(word):\n",
    "    if is_korean(word):\n",
    "        return 'KOREAN'\n",
    "    elif is_english(word):\n",
    "        return 'ENGLISH'\n",
    "    else:\n",
    "        return 'OTHER'\n",
    "\n",
    "def count_word_level_language_switches(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', str(text))\n",
    "    langs = [classify_word_regex(w) for w in words if classify_word_regex(w) in ['KOREAN', 'ENGLISH']]\n",
    "    return sum(1 for i in range(len(langs)-1) if langs[i] != langs[i+1])\n",
    "\n",
    "# Alliteration (English only, as before)\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "cmu_dict = cmudict.dict()\n",
    "def get_first_phoneme(word):\n",
    "    word = word.lower()\n",
    "    if word in cmu_dict:\n",
    "        return cmu_dict[word][0][0]\n",
    "    return None\n",
    "def count_alliterations(text):\n",
    "    words = nltk.word_tokenize(str(text))\n",
    "    phonemes = [get_first_phoneme(word) for word in words]\n",
    "    filtered = [ph for ph in phonemes if ph is not None]\n",
    "    count = 0\n",
    "    for i in range(len(filtered)-1):\n",
    "        if filtered[i] == filtered[i+1]:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Repetitions (word/phrase/sentence)\n",
    "def count_consecutive_repetitions(text, max_ngram=5):\n",
    "    tokens = str(text).split()\n",
    "    total_rep = 0\n",
    "    rep_dict = {}\n",
    "    i = 0\n",
    "    n_tokens = len(tokens)\n",
    "    while i < n_tokens:\n",
    "        found_repeat = False\n",
    "        for n in range(max_ngram, 0, -1):\n",
    "            if i + n > n_tokens:\n",
    "                continue\n",
    "            current_ngram = tokens[i:i+n]\n",
    "            rep_count = 0\n",
    "            j = i + n\n",
    "            while j + n <= n_tokens and tokens[j:j+n] == current_ngram:\n",
    "                rep_count += 1\n",
    "                j += n\n",
    "            if rep_count > 0:\n",
    "                phrase = \" \".join(current_ngram)\n",
    "                total_rep += rep_count\n",
    "                rep_dict[phrase] = rep_dict.get(phrase, 0) + rep_count\n",
    "                i = j\n",
    "                found_repeat = True\n",
    "                break\n",
    "        if not found_repeat:\n",
    "            i += 1\n",
    "    # Sentence-level\n",
    "    sentences = sent_tokenize(str(text))\n",
    "    i = 0\n",
    "    n_sent = len(sentences)\n",
    "    while i < n_sent - 1:\n",
    "        rep_count = 0\n",
    "        j = i + 1\n",
    "        while j < n_sent and sentences[j].strip() == sentences[i].strip():\n",
    "            rep_count += 1\n",
    "            j += 1\n",
    "        if rep_count > 0:\n",
    "            phrase = sentences[i].strip()\n",
    "            total_rep += rep_count\n",
    "            rep_dict[phrase] = rep_dict.get(phrase, 0) + rep_count\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    return total_rep, rep_dict\n",
    "\n",
    "def count_repetitions(text):\n",
    "    return count_consecutive_repetitions(text, max_ngram=5)\n",
    "\n",
    "# Filler words list (customize as needed)\n",
    "filler_words = []\n",
    "with open(\"./filler.txt\", \"r\") as f:\n",
    "    # if the element is \"\", it will be skipped\n",
    "    for line in f:\n",
    "        if line.strip() == \"\":\n",
    "            continue\n",
    "        # print(line)\n",
    "        filler_words.append(line.strip())\n",
    "filler_words.sort()\n",
    "\n",
    "# Apply features\n",
    "for df in [hot100, global200]:\n",
    "    df['kor_ratio'] = df['Cleaned Lyrics'].fillna('').apply(get_korean_ratio)\n",
    "    df['eng_ratio'] = df['Cleaned Lyrics'].fillna('').apply(get_english_ratio)\n",
    "    df['filler_count'] = df['Cleaned Lyrics'].fillna('').apply(lambda x: count_filler_words(x, filler_words))\n",
    "    df['alliteration_count'] = df['Cleaned Lyrics'].fillna('').apply(count_alliterations)\n",
    "    df['repetition_count'], df['repetition_dict'] = zip(*df['Cleaned Lyrics'].fillna('').apply(count_repetitions))\n",
    "    df['Gender_enc'] = np.where(df['Gender'] == 'F', 1, 0)\n",
    "    df['Category_enc'] = np.where(df['Category'] == 'S', 1, 0)\n",
    "    df['code_switches'] = df['Cleaned Lyrics'].fillna('').apply(count_word_level_language_switches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using `xlm-roberta-base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def get_lyrics_features(X):\n",
    "    # get embeddings from xlm-roberta-base\n",
    "    tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "    model = AutoModel.from_pretrained('xlm-roberta-base')\n",
    "    embeddings = []\n",
    "    for lyrics in X:\n",
    "        inputs = tokenizer(lyrics, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def run_multiple_classifiers(X, y, indices, target_names=['Boy', 'Girl']):\n",
    "    X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(X, y, indices, stratify=y, random_state=42)\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "        \"MLP (Neural Net)\": MLPClassifier(max_iter=500),\n",
    "        \"SVM\": SVC(probability=True),\n",
    "        \"Voting Classifier\": VotingClassifier(\n",
    "            estimators=[('lr', LogisticRegression(max_iter=1000)),\n",
    "                        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "                        ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n",
    "                        ('mlp', MLPClassifier(max_iter=500))],\n",
    "            voting='soft'\n",
    "        )\n",
    "    }\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        print(f\"\\n{name}\")\n",
    "        print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'y_pred': y_pred,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'idx_test': idx_test,\n",
    "            'accuracy': acc,\n",
    "            'macro_f1': macro_f1\n",
    "        }\n",
    "    return results\n",
    "\n",
    "# Prepare your features as before\n",
    "def prepare_features(df, features=None):\n",
    "    if features is None:\n",
    "        features = ['kor_ratio', 'eng_ratio', 'filler_count', 'alliteration_count', 'repetition_count', 'code_switches']\n",
    "    X = df[features]\n",
    "    X_lyrics = get_lyrics_features(df['Cleaned Lyrics'])\n",
    "    X_pca = PCA(n_components=30).fit_transform(X_lyrics)\n",
    "    X = np.concatenate([X, X_pca], axis=1)\n",
    "    y = df['Gender_enc']\n",
    "    indices = df.index\n",
    "    return X, y, indices\n",
    "\n",
    "print(\"Hot 100 Classification with Multiple Models:\")\n",
    "X_hot, y_hot, indices_hot = prepare_features(hot100)\n",
    "hot100_results = run_multiple_classifiers(X_hot, y_hot, indices_hot)\n",
    "best_model_name_100 = max(hot100_results, key=lambda k: hot100_results[k]['macro_f1'])\n",
    "print(f\"Best model for Hot 100: {best_model_name_100} (Macro F1: {hot100_results[best_model_name_100]['macro_f1']:.3f})\")\n",
    "\n",
    "print(\"\\nGlobal 200 Classification with Multiple Models:\")\n",
    "X_global, y_global, indices_global = prepare_features(global200)\n",
    "global200_results = run_multiple_classifiers(X_global, y_global, indices_global)\n",
    "best_model_name_200 = max(global200_results, key=lambda k: global200_results[k]['macro_f1'])\n",
    "print(f\"Best model for Global 200: {best_model_name_200} (Macro F1: {global200_results[best_model_name_200]['macro_f1']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misclassification analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_misclassifications(df, test_idx, y_test, y_pred):\n",
    "    df_test = df.loc[test_idx].copy()\n",
    "    df_test['true_label'] = y_test\n",
    "    df_test['pred_label'] = y_pred\n",
    "    false_pos = df_test[(df_test['true_label'] == 0) & (df_test['pred_label'] == 1)]\n",
    "    false_neg = df_test[(df_test['true_label'] == 1) & (df_test['pred_label'] == 0)]\n",
    "    print(f\"False Positives (Boy predicted as Girl): {len(false_pos)}\")\n",
    "    print(false_pos[['artist', 'title', 'Cleaned Lyrics']].head())\n",
    "    print(f\"\\nFalse Negatives (Girl predicted as Boy): {len(false_neg)}\")\n",
    "    print(false_neg[['artist', 'title', 'Cleaned Lyrics']].head())\n",
    "    return false_pos, false_neg\n",
    "\n",
    "print(\"Hot 100 Misclassifications:\")\n",
    "hot100_fp, hot100_fn = analyze_misclassifications(\n",
    "    hot100, \n",
    "    hot100_results[best_model_name_100]['idx_test'],  # pass indices, not X_test\n",
    "    hot100_results[best_model_name_100]['y_test'],\n",
    "    hot100_results[best_model_name_100]['y_pred']\n",
    ")\n",
    "print(\"\\nGlobal 200 Misclassifications:\")\n",
    "global200_fp, global200_fn = analyze_misclassifications(\n",
    "    global200,\n",
    "    global200_results[best_model_name_200]['idx_test'],  # pass indices, not X_test\n",
    "    global200_results[best_model_name_200]['y_test'],\n",
    "    global200_results[best_model_name_200]['y_pred']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using `gte-multilingual-base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def get_lyrics_features(X):\n",
    "    # get embeddings from xlm-roberta-base\n",
    "    tokenizer = AutoTokenizer.from_pretrained('Alibaba-NLP/gte-multilingual-base', trust_remote_code=True)\n",
    "    model = AutoModel.from_pretrained('Alibaba-NLP/gte-multilingual-base', trust_remote_code=True)\n",
    "    embeddings = []\n",
    "    for lyrics in X:\n",
    "        inputs = tokenizer(lyrics, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def run_multiple_classifiers(X, y, indices, target_names=['Boy', 'Girl']):\n",
    "    X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(X, y, indices, stratify=y, random_state=42)\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "        \"MLP (Neural Net)\": MLPClassifier(max_iter=500),\n",
    "        \"SVM\": SVC(probability=True),\n",
    "        \"Voting Classifier\": VotingClassifier(\n",
    "            estimators=[('lr', LogisticRegression(max_iter=1000)),\n",
    "                        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "                        ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n",
    "                        ('mlp', MLPClassifier(max_iter=500))],\n",
    "            voting='soft'\n",
    "        )\n",
    "    }\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        print(f\"\\n{name}\")\n",
    "        print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'y_pred': y_pred,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'idx_test': idx_test,\n",
    "            'accuracy': acc,\n",
    "            'macro_f1': macro_f1\n",
    "        }\n",
    "    return results\n",
    "\n",
    "# Prepare your features as before\n",
    "def prepare_features(df, features=None):\n",
    "    if features is None:\n",
    "        features = ['kor_ratio', 'eng_ratio', 'filler_count', 'alliteration_count', 'repetition_count', 'code_switches']\n",
    "    X = df[features]\n",
    "    X_lyrics = get_lyrics_features(df['Cleaned Lyrics'])\n",
    "    X_pca = PCA(n_components=30).fit_transform(X_lyrics)\n",
    "    X = np.concatenate([X, X_pca], axis=1)\n",
    "    y = df['Gender_enc']\n",
    "    indices = df.index\n",
    "    return X, y, indices\n",
    "\n",
    "print(\"Hot 100 Classification with Multiple Models:\")\n",
    "X_hot, y_hot, indices_hot = prepare_features(hot100)\n",
    "hot100_results = run_multiple_classifiers(X_hot, y_hot, indices_hot)\n",
    "best_model_name_100 = max(hot100_results, key=lambda k: hot100_results[k]['macro_f1'])\n",
    "print(f\"Best model for Hot 100: {best_model_name_100} (Macro F1: {hot100_results[best_model_name_100]['macro_f1']:.3f})\")\n",
    "\n",
    "print(\"\\nGlobal 200 Classification with Multiple Models:\")\n",
    "X_global, y_global, indices_global = prepare_features(global200)\n",
    "global200_results = run_multiple_classifiers(X_global, y_global, indices_global)\n",
    "best_model_name_200 = max(global200_results, key=lambda k: global200_results[k]['macro_f1'])\n",
    "print(f\"Best model for Global 200: {best_model_name_200} (Macro F1: {global200_results[best_model_name_200]['macro_f1']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_misclassifications(df, test_idx, y_test, y_pred):\n",
    "    df_test = df.loc[test_idx].copy()\n",
    "    df_test['true_label'] = y_test\n",
    "    df_test['pred_label'] = y_pred\n",
    "    false_pos = df_test[(df_test['true_label'] == 0) & (df_test['pred_label'] == 1)]\n",
    "    false_neg = df_test[(df_test['true_label'] == 1) & (df_test['pred_label'] == 0)]\n",
    "    print(f\"False Positives (Boy predicted as Girl): {len(false_pos)}\")\n",
    "    print(false_pos[['artist', 'title', 'Cleaned Lyrics']].head())\n",
    "    print(f\"\\nFalse Negatives (Girl predicted as Boy): {len(false_neg)}\")\n",
    "    print(false_neg[['artist', 'title', 'Cleaned Lyrics']].head())\n",
    "    return false_pos, false_neg\n",
    "\n",
    "print(\"Hot 100 Misclassifications:\")\n",
    "hot100_fp, hot100_fn = analyze_misclassifications(\n",
    "    hot100, \n",
    "    hot100_results[best_model_name_100]['idx_test'],  # pass indices, not X_test\n",
    "    hot100_results[best_model_name_100]['y_test'],\n",
    "    hot100_results[best_model_name_100]['y_pred']\n",
    ")\n",
    "print(\"\\nGlobal 200 Misclassifications:\")\n",
    "global200_fp, global200_fn = analyze_misclassifications(\n",
    "    global200,\n",
    "    global200_results[best_model_name_200]['idx_test'],  # pass indices, not X_test\n",
    "    global200_results[best_model_name_200]['y_test'],\n",
    "    global200_results[best_model_name_200]['y_pred']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratio analysis in lyrics\n",
    "- Analyzing the ratio of Korean and English words in the lyrics of hot-100.\n",
    "- Analyzing the ratio of Korean and English words in the lyrics of global-200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_phrase_stats(df, group_col='Category', gender_col='Gender'):\n",
    "    # Korean/English ratio\n",
    "    print(df.groupby([group_col, gender_col])[['kor_ratio', 'eng_ratio']].mean())\n",
    "    # Most common phrases (bigrams)\n",
    "    for (g, gender), group_df in df.groupby([group_col, gender_col]):\n",
    "        all_lyrics = ' '.join(group_df['Cleaned Lyrics'].dropna())\n",
    "        bigrams = [' '.join(pair) for pair in zip(all_lyrics.split()[:-1], all_lyrics.split()[1:])]\n",
    "        bigram_counts = Counter(bigrams)\n",
    "        print(f\"\\nMost common bigrams for {g} {gender}:\")\n",
    "        print(bigram_counts.most_common(10))\n",
    "\n",
    "print(\"Hot 100 Group/Solo & Gender Stats:\")\n",
    "group_phrase_stats(hot100)\n",
    "print(\"\\nGlobal 200 Group/Solo & Gender Stats:\")\n",
    "group_phrase_stats(global200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis\n",
    "- Using statistical analysis to analyze the data.\n",
    "- Using t-test to analyze the data.\n",
    "- Using Mann-Whitney U test to analyze the data.\n",
    "- Using Shapiro-Wilk test to analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do boy groups and girl groups differ in the amount of code-switching in their lyrics?\n",
    "# Do boy groups and girl groups differ in the proportion of Korean lyrics in their songs?\n",
    "\n",
    "from scipy.stats import ttest_ind, shapiro, mannwhitneyu\n",
    "\n",
    "def run_statistical_analysis(df, feature, group_col='Gender_enc'):\n",
    "    group0 = df[df[group_col] == 0][feature].dropna()\n",
    "    group1 = df[df[group_col] == 1][feature].dropna()\n",
    "    print(f\"\\nAnalyzing feature: {feature}\")\n",
    "    print(f\"Group 0 mean: {group0.mean():.2f}\")\n",
    "    print(f\"Group 1 mean: {group1.mean():.2f}\")\n",
    "    # Welch's t-test\n",
    "    t_stat, p_val = ttest_ind(group0, group1, equal_var=False)\n",
    "    print(f\"Welch t-test: t-statistic: {t_stat:.2f}, p-value: {p_val:.4f}\")\n",
    "    # Shapiro-Wilk normality test\n",
    "    stat0, p0 = shapiro(group0)\n",
    "    stat1, p1 = shapiro(group1)\n",
    "    print(f\"Group 0 normality p-value: {p0:.4f}\")\n",
    "    print(f\"Group 1 normality p-value: {p1:.4f}\")\n",
    "    # Mann-Whitney U test\n",
    "    u_stat, p_u = mannwhitneyu(group0, group1, alternative='two-sided')\n",
    "    print(f\"Mann-Whitney U statistic: {u_stat:.2f}, p-value: {p_u:.4f}\")\n",
    "\n",
    "# Example usage:\n",
    "run_statistical_analysis(hot100, 'code_switches')\n",
    "run_statistical_analysis(hot100, 'kor_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do boy groups and girl groups differ in the amount of code-switching in their lyrics?\n",
    "# Do boy groups and girl groups differ in the proportion of Korean lyrics in their songs?\n",
    "\n",
    "from scipy.stats import ttest_ind, shapiro, mannwhitneyu\n",
    "\n",
    "def run_statistical_analysis(df, feature, group_col='Gender_enc'):\n",
    "    group0 = df[df[group_col] == 0][feature].dropna()\n",
    "    group1 = df[df[group_col] == 1][feature].dropna()\n",
    "    print(f\"\\nAnalyzing feature: {feature}\")\n",
    "    print(f\"Group 0 mean: {group0.mean():.2f}\")\n",
    "    print(f\"Group 1 mean: {group1.mean():.2f}\")\n",
    "    # Welch's t-test\n",
    "    t_stat, p_val = ttest_ind(group0, group1, equal_var=False)\n",
    "    print(f\"Welch t-test: t-statistic: {t_stat:.2f}, p-value: {p_val:.4f}\")\n",
    "    # Shapiro-Wilk normality test\n",
    "    stat0, p0 = shapiro(group0)\n",
    "    stat1, p1 = shapiro(group1)\n",
    "    print(f\"Group 0 normality p-value: {p0:.4f}\")\n",
    "    print(f\"Group 1 normality p-value: {p1:.4f}\")\n",
    "    # Mann-Whitney U test\n",
    "    u_stat, p_u = mannwhitneyu(group0, group1, alternative='two-sided')\n",
    "    print(f\"Mann-Whitney U statistic: {u_stat:.2f}, p-value: {p_u:.4f}\")\n",
    "\n",
    "# Example usage:\n",
    "run_statistical_analysis(global200, 'code_switches')\n",
    "run_statistical_analysis(global200, 'kor_ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **RQ1:** Do boy groups and girl groups differ in the amount of code-switching in their lyrics?\n",
    "- **RQ2:** Do boy groups and girl groups differ in the proportion of Korean lyrics in their songs?\n",
    "\n",
    "\n",
    "## **Summary of Results**\n",
    "\n",
    "### **A. Code-Switching**\n",
    "\n",
    "- **Means:**  \n",
    "  - Boy groups: 19.05  \n",
    "  - Girl groups: 16.96\n",
    "- **Welch’s t-test:**  \n",
    "  - t = 0.41, p = 0.6862\n",
    "- **Shapiro-Wilk normality:**  \n",
    "  - Both p < 0.001 (not normal)\n",
    "- **Mann-Whitney U test:**  \n",
    "  - U = 555.50, p = 0.6109\n",
    "\n",
    "**Interpretation:**  \n",
    "- The mean code-switching count is slightly higher for boy groups, but the difference is **not statistically significant** (p > 0.05) in both the t-test and the Mann-Whitney U test.\n",
    "- The distributions are not normal, so the Mann-Whitney U test is more reliable.\n",
    "- **Conclusion:** We fail to reject the null hypothesis; there is no significant difference in code-switching frequency between boy and girl groups.\n",
    "\n",
    "---\n",
    "\n",
    "### **B. Korean Lyric Ratio**\n",
    "\n",
    "- **Means:**  \n",
    "  - Boy groups: 0.24  \n",
    "  - Girl groups: 0.11\n",
    "- **Welch’s t-test:**  \n",
    "  - t = 2.68, p = 0.0097\n",
    "- **Shapiro-Wilk normality:**  \n",
    "  - Both p < 0.001 (not normal)\n",
    "- **Mann-Whitney U test:**  \n",
    "  - U = 646.00, p = 0.0796\n",
    "\n",
    "**Interpretation:**  \n",
    "- The mean Korean lyric ratio is higher for boy groups (0.24) than for girl groups (0.11).\n",
    "- The t-test suggests a significant difference (p = 0.0097), but the data are not normal.\n",
    "- The Mann-Whitney U test, which is more appropriate here, gives p = 0.0796 (**not significant at the 0.05 level**).\n",
    "- **Conclusion:** We fail to reject the null hypothesis; there is no statistically significant difference in the proportion of Korean lyrics between boy and girl groups, although the difference is numerically notable and approaches significance.\n",
    "\n",
    "---\n",
    "\n",
    "### How i might write this in the paper\n",
    "\n",
    "We compared both the frequency of code-switching and the proportion of Korean lyrics between boy and girl groups in Hot 100 songs. The mean code-switching count was 19.05 for boy groups and 16.96 for girl groups, while the mean Korean lyric ratio was 0.24 for boy groups and 0.11 for girl groups. Welch’s t-test suggested a significant difference in Korean lyric ratio (t = 2.68, p = 0.0097), but not in code-switching (t = 0.41, p = 0.6862). However, the Shapiro-Wilk test indicated that the distributions of both features were not normal (p < 0.001 for all groups). Therefore, we relied on the non-parametric Mann-Whitney U test, which found no significant difference for either code-switching (U = 555.50, p = 0.6109) or Korean lyric ratio (U = 646.00, p = 0.0796). Thus, we fail to reject the null hypothesis and conclude that there is no statistically significant difference between boy and girl groups in either code-switching frequency or the proportion of Korean lyrics in Hot 100 songs, although the difference in Korean lyric ratio approaches significance.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- **Statistical best practice:** When normality is violated, non-parametric tests are preferred.\n",
    "- **\"Fail to reject the null hypothesis\":** This is the correct language when p > 0.05.\n",
    "- **Reporting means:** Shows the numerical trend, even if not statistically significant.\n",
    "- **Reporting both tests:** Demonstrates rigor and transparency.\n",
    "\n",
    "\n",
    "\"The difference in Korean lyric ratio approaches statistical significance (p = 0.0796), suggesting a possible trend for boy groups to use more Korean lyrics than girl groups, but this did not reach the conventional threshold for significance.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there a difference in the proportion of Korean lyrics (kor_ratio) between Hot 100 and Global 200 charts for boy and girl groups?\n",
    "from scipy.stats import ttest_ind, shapiro, mannwhitneyu\n",
    "\n",
    "def compare_kor_ratio_between_charts(hot100, global200, gender_enc, gender_label):\n",
    "    # Filter by gender\n",
    "    hot_kor = hot100[hot100['Gender_enc'] == gender_enc]['kor_ratio'].dropna()\n",
    "    global_kor = global200[global200['Gender_enc'] == gender_enc]['kor_ratio'].dropna()\n",
    "    print(f\"\\n{gender_label} groups:\")\n",
    "    print(f\"  Hot 100 mean kor_ratio: {hot_kor.mean():.3f}\")\n",
    "    print(f\"  Global 200 mean kor_ratio: {global_kor.mean():.3f}\")\n",
    "    # Welch's t-test\n",
    "    t_stat, p_val = ttest_ind(hot_kor, global_kor, equal_var=False)\n",
    "    print(f\"  Welch t-test: t-statistic: {t_stat:.2f}, p-value: {p_val:.4f}\")\n",
    "    # Shapiro-Wilk normality\n",
    "    p_hot = shapiro(hot_kor)[1]\n",
    "    p_global = shapiro(global_kor)[1]\n",
    "    print(f\"  Hot 100 normality p-value: {p_hot:.4f}\")\n",
    "    print(f\"  Global 200 normality p-value: {p_global:.4f}\")\n",
    "    # Mann-Whitney U test\n",
    "    u_stat, p_u = mannwhitneyu(hot_kor, global_kor, alternative='two-sided')\n",
    "    print(f\"  Mann-Whitney U statistic: {u_stat:.2f}, p-value: {p_u:.4f}\")\n",
    "\n",
    "# Run for both genders\n",
    "compare_kor_ratio_between_charts(hot100, global200, gender_enc=0, gender_label=\"Boy\")\n",
    "compare_kor_ratio_between_charts(hot100, global200, gender_enc=1, gender_label=\"Girl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research Question:\n",
    "- Is there a difference in the proportion of Korean lyrics (kor_ratio) between Hot 100 and Global 200 charts for boy and girl groups?\n",
    "\n",
    "Hypotheses (for each gender):\n",
    "- Null hypothesis (H₀):\n",
    "    The mean Korean lyric ratio is the same for Hot 100 and Global 200 songs.\n",
    "    (μ_hot100 = μ_global200)\n",
    "- Alternative hypothesis (H₁):\n",
    "    The mean Korean lyric ratio is different between Hot 100 and Global 200 songs.\n",
    "    (μ_hot100 ≠ μ_global200)\n",
    "--\n",
    "\n",
    "### **A. Boy Groups**\n",
    "\n",
    "- **Hot 100 mean kor_ratio:** 0.236  \n",
    "- **Global 200 mean kor_ratio:** 0.238  \n",
    "- **Welch’s t-test:** t = -0.05, p = 0.9599  \n",
    "- **Shapiro-Wilk normality:** Both p < 0.001 (not normal)\n",
    "- **Mann-Whitney U test:** U = 1234.00, p = 0.8717\n",
    "\n",
    "**Interpretation:**  \n",
    "- The mean Korean lyric ratio is nearly identical for boy groups on both charts.\n",
    "- Both the t-test and Mann-Whitney U test show **no significant difference** (p ≫ 0.05).\n",
    "- **Conclusion:** We fail to reject the null hypothesis; there is no significant difference in the proportion of Korean lyrics for boy groups between the Hot 100 and Global 200 charts.\n",
    "\n",
    "### **B. Girl Groups**\n",
    "\n",
    "- **Hot 100 mean kor_ratio:** 0.112  \n",
    "- **Global 200 mean kor_ratio:** 0.171  \n",
    "- **Welch’s t-test:** t = -2.00, p = 0.0498  \n",
    "- **Shapiro-Wilk normality:** Both p < 0.001 (not normal)\n",
    "- **Mann-Whitney U test:** U = 1076.00, p = 0.1995\n",
    "\n",
    "**Interpretation:**  \n",
    "- The mean Korean lyric ratio is higher for girl groups on the Global 200 (0.171) than on the Hot 100 (0.112).\n",
    "- The t-test suggests a borderline significant difference (p = 0.0498), but the data are not normal.\n",
    "- The Mann-Whitney U test, which is more appropriate, gives p = 0.1995 (**not significant**).\n",
    "- **Conclusion:** We fail to reject the null hypothesis; there is no statistically significant difference in the proportion of Korean lyrics for girl groups between the Hot 100 and Global 200 charts, although the numerical difference is notable.\n",
    "\n",
    "\n",
    "\n",
    "## 3. **How i might write this in the paper**\n",
    "\n",
    "We compared the proportion of Korean lyrics (kor_ratio) between Hot 100 and Global 200 charts for both boy and girl groups. For boy groups, the mean kor_ratio was nearly identical (Hot 100: 0.236, Global 200: 0.238), and neither Welch’s t-test (p = 0.9599) nor the Mann-Whitney U test (p = 0.8717) indicated a significant difference. For girl groups, the mean kor_ratio was higher on the Global 200 (0.171) than on the Hot 100 (0.112). While Welch’s t-test suggested a borderline significant difference (p = 0.0498), the Shapiro-Wilk test indicated that the data were not normally distributed (p < 0.001 for both groups). The Mann-Whitney U test, which does not assume normality, found no significant difference (p = 0.1995). Thus, we fail to reject the null hypothesis and conclude that there is no statistically significant difference in the proportion of Korean lyrics between the two charts for either boy or girl groups, although the difference for girl groups is numerically notable.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Why This Interpretation?**\n",
    "\n",
    "- **Statistical best practice:** When normality is violated, rely on the non-parametric test.\n",
    "- **\"Fail to reject the null hypothesis\":** Correct language when p > 0.05.\n",
    "- **Reporting means:** Shows the numerical trend, even if not statistically significant.\n",
    "- **Reporting both tests:** Demonstrates rigor and transparency.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of kor_ratio by chart and gender\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hot100['Chart'] = 'Hot 100'\n",
    "global200['Chart'] = 'Global 200'\n",
    "combined = pd.concat([hot100, global200])\n",
    "\n",
    "sns.boxplot(x='Chart', y='kor_ratio', hue='Gender_enc', data=combined, showfliers=False)\n",
    "plt.title('Korean Lyric Ratio by Chart and Gender')\n",
    "plt.xlabel('Chart')\n",
    "plt.ylabel('Korean Lyric Ratio')\n",
    "plt.legend(title='Gender (0=Boy, 1=Girl)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7420715,
     "sourceId": 11863329,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kpop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
